{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc4ceb91-7a07-4962-99f1-be0d0b77ab36",
   "metadata": {},
   "source": [
    "## 1.What are the objectives  of using Selective Search in R-CNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001a7867-bba1-4b8e-9cd9-c7785a1af799",
   "metadata": {},
   "source": [
    "## Efficient Region Proposal Generation:\n",
    "## R-CNN requires extracting region proposals from images for object detection. Exhaustive search, which scans the entire image with sliding windows of various sizes, is a straightforward but computationally expensive approach. Selective Search provides a more efficient alternative by:\n",
    "##  Over-segmenting the image: It uses a graph-based segmentation method to divide the image into many small, initial segments based on pixel intensity.\n",
    "##  Merging similar segments: It iteratively combines these initial segments based on their similarity in color, texture, and size, forming larger and more meaningful region proposals.\n",
    "\n",
    "## High Recall Rate:\n",
    "## Selective Search aims to achieve a high recall rate, meaning it should identify most of the actual objects in the image. This is achieved by:\n",
    "## Generating diverse proposals: The algorithm considers various combinations of initial segments during merging, leading to a wide range of potential object proposals.\n",
    "## Hierarchical merging: Merging smaller segments into larger ones allows capturing objects of varying sizes and aspect ratios.\n",
    "\n",
    "## Reducing Computational Cost.\n",
    "##  Improving Detection Performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf12514-45d8-484a-a65e-5b9448bb70ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3d561ea-8a79-4b22-9928-4f085537c920",
   "metadata": {},
   "source": [
    "## 2.Explain the flowing phases involved in R-CNN:\n",
    "## a.Region proposal\n",
    "## b.warping and resizing\n",
    "## c.Pre trained CNN architecture\n",
    "## d.pre trained svm architecture\n",
    "## e.clean up\n",
    "## f.implementation of bounding box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f99e12-3338-49f7-bea9-5209adb68a91",
   "metadata": {},
   "source": [
    "## R-CNN is a two-stage object detection algorithm that involves the following phases:\n",
    "# Region Proposal: Selective Search: This algorithm is used to efficiently identify potential object regions in the image. It works by segmenting the image into small regions based on color and texture similarity, then iteratively merging similar regions to form larger proposals.\n",
    "## Exhaustive Search: While less efficient, exhaustive search involves sliding windows of different sizes across the entire image to generate potential object regions.\n",
    "\n",
    "#  Warping and Resizing: The extracted region proposals are normalized to a fixed size, typically 227x227 pixels. This ensures compatibility with the pre-trained CNN architecture.\n",
    "## Warping techniques like bilinear interpolation are used to avoid distortion while resizing.\n",
    "\n",
    "# Pre-trained CNN Architecture: The normalized region proposals are fed into a pre-trained CNN (e.g., AlexNet) to extract feature vectors. These features capture the object's appearance information.\n",
    "## The CNN architecture is pre-trained on a large dataset of labeled images, allowing it to learn general features useful for object recognition.\n",
    "\n",
    "# Clean Up: The SVM outputs confidence scores for each object class. Low-scoring proposals and those with overlapping bounding boxes are discarded.\n",
    "\n",
    "#  Implementation of Bounding Box: The remaining high-scoring proposals are used to refine the object's location and size. This is done using bounding box regression, which predicts adjustments to the proposals' bounding boxes based on the extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fed9b8-c7ec-4469-a07a-13e6c47a65fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f114c9f-9e0d-4ba7-bf8a-a8218352dfd5",
   "metadata": {},
   "source": [
    "## 3.What are the possible pre trained CNNs we can use in Pre trained CSS architecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a977acfc-4f4b-42b8-b128-8ecf0bd37a03",
   "metadata": {},
   "source": [
    "## The application of pre-trained CNNs in pre-trained CSS architecture (PT-CSS) is a very interesting and emerging area of research. However, it's important to clarify that currently, pre-trained CNNs directly applied to PT-CSS are not quite common or readily available. This is because CSS deals with textual code, while CNNs are traditionally used for processing images and other visual data.\n",
    "\n",
    "# Feature extraction from visuals: Image analysis for code layout.\n",
    "# Textual representation with visual analogies: Learning visual embeddings for keywords.\n",
    "## Domain adaptation with cross-modal transfer:Leveraging pre-trained visual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4dcca0-8484-45fc-bde9-ff4d21f33b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5c76eee-28e3-4cf1-ac8f-b78c8a899312",
   "metadata": {},
   "source": [
    "## 4.How is SVM implemented in the R-CNN framework ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d9610d-0f19-4140-89f2-b0264236a3c9",
   "metadata": {},
   "source": [
    "##  The role of SVMs in the original R-CNN framework (released in 2013) involved classification, specifically of candidate object regions that were generated by a separate \"region proposal\" algorithm like selective search. Here's how it worked:\n",
    "\n",
    "## Region Proposals.\n",
    "## Feature Extraction.\n",
    "## Classification with SVMs.\n",
    "## Bounding Box Regression.\n",
    "## Final Detection.\n",
    "## Limitations of SVMs in R-CNN.\n",
    "## Later R-CNN variants have largely moved away from using SVMs for classification.\n",
    "## While SVMs played a crucial role in the original R-CNN framework, their usage has been diminished in favor of more efficient and scalable deep learning approaches in subsequent R-CNN variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e137f3-bece-449e-ad66-5d97b33b8390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2807ba3-ac49-41fd-a1a7-f6187769c7bf",
   "metadata": {},
   "source": [
    "## 5.How does Non-maximum Suppression work ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89136980-cb0a-4639-8974-df4eec9d675f",
   "metadata": {},
   "source": [
    "## Non-maximum suppression (NMS) is a post-processing technique commonly used in object detection tasks to remove redundant detections and keep only the most likely ones. It's especially helpful when dealing with overlapping bounding boxes proposed by object detection models. Here's how it works:\n",
    "# Imagine: You have a scene with multiple objects, like three cars parked close together.\n",
    "# NMS helps choose the most accurate boxes:\n",
    "## Sort bounding boxes by confidence score: Rank the proposed bounding boxes based on how confident the model is that they contain an object. Higher confidence scores indicate a greater likelihood of a true detection.\n",
    "## Iterate through sorted boxes: Starting with the box with the highest confidence score, follow these steps: Keep the current box. Check for overlap with remaining boxes.\n",
    "\n",
    "# Benefits of NMS:\n",
    "## Reduces false positives: Eliminates redundant detections caused by overlapping bounding boxes, improving the precision of your object detection system.\n",
    "## Simplifies downstream tasks: Provides a cleaner set of detections for further analysis, tracking, or visualization tasks.\n",
    "\n",
    "# NMS has different variations and parameters:\n",
    "## The choice of IoU threshold determines the level of overlap allowed before suppression. A higher threshold leads to fewer detections but potentially excludes valid objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a88503b-4744-4e76-a8df-7b60f5ccc0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dbc0641-d155-4c10-9c6b-ba52ec98868a",
   "metadata": {},
   "source": [
    "## 6.How Fast R-CNN is better than R-CNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382e4181-bacf-4cd5-a046-286580340d88",
   "metadata": {},
   "source": [
    "## Both R-CNN and Fast R-CNN are object detection algorithms, but Fast R-CNN offers several significant advantages over its predecessor:\n",
    "\n",
    "# Speed:\n",
    "## R-CNN: The original R-CNN is incredibly slow. For each potential object location in an image, it extracts features, classifies them, and refines the bounding box. This process is repeated hundreds or thousands of times per image, making R-CNN impractical for real-time applications.\n",
    "\n",
    "## Fast R-CNN: This version streamlines the process by sharing feature computations across all potential object locations. Instead of extracting features for each proposal individually, it extracts features for the entire image once and then applies those features to each proposal. This significantly reduces the processing time, making Fast R-CNN several times faster than R-CNN.\n",
    "\n",
    "# Accuracy:\n",
    "## While Fast R-CNN is faster than R-CNN, it can sometimes be slightly less accurate. This is because it uses a simpler bounding box regression technique.\n",
    "\n",
    "# Training:\n",
    "## Training R-CNN is a complex and time-consuming process. It requires training separate SVMs for each object class, which can be computationally expensive.\n",
    "## Fast R-CNN uses a unified training objective for both region proposal and classification, making it much faster and easier to train.\n",
    "\n",
    "# Overall:\n",
    "\n",
    "## Fast R-CNN represents a significant improvement over R-CNN in terms of speed and efficiency. While it may not be perfect, it is a much more practical and widely used object detection algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddb2fd9-dd7c-4352-91d3-fe123e203790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da231fe7-ab4b-4bfe-acbf-dce04e870249",
   "metadata": {},
   "source": [
    "## 7.Using mathematical intuition, explain ROI pooling in Fast R-CNN ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c25c085-72d7-404e-888c-326ebd66b1fb",
   "metadata": {},
   "source": [
    "## ROI Pooling in Fast R-CNN: A Mathematical Intuition\n",
    "\n",
    "## Fast R-CNN's speed advantage over R-CNN hinges on its efficient handling of Region of Interest (ROI) features. Here's a breakdown of ROI pooling using mathematical intuition:\n",
    "## Imagine:\n",
    "## .You have an image and a set of proposed regions (ROIs) where objects might be.\n",
    "## .Each ROI is a rectangular box with specific coordinates.\n",
    "## .You want to extract features from these ROIs using a pre-trained convolutional neural network (CNN).\n",
    "\n",
    "# The problem:\n",
    "## .Directly feeding each ROI to the CNN is computationally expensive.\n",
    "## .Naive resizing of each ROI to the CNN's input size might distort spatial information.\n",
    "\n",
    "## .ROI pooling solves this with a clever trick:\n",
    "## Divide each ROI into a grid of smaller sub-regions: Think of a grid of equal squares within the ROI. The size of the grid (number of rows and columns) is a hyperparameter you can choose.\n",
    "## Max pooling within each sub-region: Within each sub-region, find the maximum activation value of the CNN feature map at that location. This captures the most prominent feature in that area.\n",
    "## Reshape the output: The resulting output is a fixed-size tensor (e.g., 7x7), regardless of the original ROI size. This allows all ROIs to be fed into the same fully-connected layers for classification and regression.\n",
    "\n",
    "# Mathematical intuition:\n",
    "## Max pooling: This operation is essentially saying, \"within this small area, what is the most significant feature?\" This helps capture the dominant characteristics of an object within the ROI.\n",
    "## Grid size: The size of the grid determines the level of detail extracted. A larger grid captures more fine-grained information, while a smaller grid focuses on broader features. Choosing the right size depends on your task and dataset.\n",
    "## Fixed output size: This allows efficient processing of all ROIs by the subsequent layers, regardless of their original size or aspect ratio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25442d0c-217b-4550-8b51-0b0f1353a9b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4905cfad-b50e-4156-b1ee-b7b5185a2b49",
   "metadata": {},
   "source": [
    "## 8.Explain the following prcesses:\n",
    "## a.ROI projection\n",
    "## b.ROI pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447fef6e-c066-4bd0-8646-a4627ac8e376",
   "metadata": {},
   "source": [
    "## ROI Projection:\n",
    "## Imagine you have an image and a set of proposed regions of interest (ROIs) identified by a separate algorithm. These ROIs might be bounding boxes around potential objects. However, the size of these ROIs may not match the input size of the convolutional neural network (CNN) used for feature extraction in Fast R-CNN.\n",
    "## Warped Perspective: Think of it as stretching or shrinking the ROI like a rubber sheet to fit the aspect ratio and dimensions of the CNN's input layer. This process preserves the spatial relationships within the ROI while adapting it to the network's requirements.\n",
    "\n",
    "##  Bilinear Interpolation: To fill in the gaps created by stretching or shrinking, neighboring pixels in the original image are interpolated using a weighted average. This ensures smooth transitions and avoids pixelated artifacts in the warped ROI.\n",
    "\n",
    "## ROI Pooling:Now you have ROIs warped to the CNN's input size, but not all features within the ROI are equally important. ROI pooling efficiently extracts the most relevant information:\n",
    "## Grid Division: Imagine dividing the warped ROI into a grid of smaller sub-regions. The size of this grid is a hyperparameter you can choose based on your task and dataset.\n",
    "##  Max Pooling (or Alternative): Within each sub-region, the dominant feature value is extracted. Traditionally, max pooling is used, meaning the highest activation value from the CNN's feature map at that location is chosen. This captures the most prominent feature within the small area. However, other pooling strategies like average pooling or bilinear interpolation can also be used depending on the desired level of detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b74e38-014f-4744-a2dd-31e2020b3d7e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3cb3833-4c49-4c8c-bdd9-39dfe000f4db",
   "metadata": {},
   "source": [
    "##  10.What major changes in Faster R-CNN compared to Fast R-CNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dd5f77-7e6a-4d5f-b10d-686abc50ef80",
   "metadata": {},
   "source": [
    "## While Fast R-CNN revolutionized object detection by introducing efficient ROI processing and unified training, Faster R-CNN further elevated the game by addressing another bottleneck: region proposal generation. Here's a breakdown of the key changes:\n",
    "# Region Proposal Network (RPN):\n",
    "## This is the game-changer! Replacing the external \"selective search\" algorithm used in Fast R-CNN, Faster R-CNN integrates a deep learning-based RPN directly into the network.\n",
    "## The RPN shares convolutional features with the rest of the network, drastically improving efficiency and speed.\n",
    "## It predicts both bounding boxes and objectness scores for potential regions, providing more accurate and faster region proposals.\n",
    "\n",
    "# Anchor Boxes:\n",
    "## Faster R-CNN uses a set of pre-defined \"anchor boxes\" of different sizes and aspect ratios at each location in the feature map.\n",
    "## The RPN predicts adjustments to these anchor boxes to refine their positions and sizes for better object localization.\n",
    "# Multi-task Loss Function:\n",
    "## Both region proposal and object detection are trained jointly using a single loss function.\n",
    "## This encourages the network to learn features that are not only good for classification but also for accurate region proposals.\n",
    "#  Faster Training and Inference:\n",
    "## By integrating region proposal into the network and sharing features, Faster R-CNN achieves significantly faster training and inference times compared to Fast R-CNN.\n",
    "## This opens doors for real-time object detection applications and resource-constrained environments.\n",
    "\n",
    "# Improved Accuracy:\n",
    "## The RPN's ability to learn better region proposals leads to more accurate object detection overall.\n",
    "## This makes Faster R-CNN a powerful tool for tasks requiring high precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e25b9c5-592e-4f03-bd69-10eeb2bdd972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "713c3545-a754-42fa-8854-ef77b05468d3",
   "metadata": {},
   "source": [
    "## 11.Explain the concept of Anchor box."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d4fa8a-06d0-4cdc-b3c1-ce29f3b19659",
   "metadata": {},
   "source": [
    "## In object detection, anchor boxes play a crucial role in guiding and refining the predictions made by the model. Imagine them as pre-defined templates, like building blocks, that the model uses to identify and localize objects in an image.\n",
    "##  Why Anchor Boxes?\n",
    "## Efficiency: Instead of searching for objects across the entire feature map at once, anchor boxes provide a starting point for the network. This reduces the computational complexity of the task.\n",
    "## Multiple objects: Anchor boxes with different sizes and aspect ratios cater to the possibility of finding objects of various shapes and sizes within an image.\n",
    "\n",
    "## How do they work?\n",
    "## Placement: Anchor boxes are placed at regular intervals on the feature map, often across different spatial scales and aspect ratios. This ensures they cover potential object locations across the entire image.\n",
    "## Refinement: The network predicts adjustments to the size and position of these anchor boxes based on the features it extracts from them. This process refines the initial guesses to better match the actual objects present in the image.\n",
    "\n",
    "## Benefits of Anchor Boxes:\n",
    "## Improved accuracy: By providing a starting point and focusing the search, anchor boxes can improve the network's ability to localize objects accurately.\n",
    "## Faster training and inference: Compared to searching the entire feature map, analyzing predefined anchor boxes can be computationally more efficient, leading to faster training and inference times.\n",
    "\n",
    "## Things to remember:\n",
    "## The choice of anchor box sizes and aspect ratios is crucial and can affect the network's performance. Different tasks may require different sets of anchors.\n",
    "## Anchor boxes are not perfect, and the network might still miss objects that fall outside their predefined shapes or sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4172f3cc-2cc4-48a6-8d82-c649ce718570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "135efa02-b75e-40e8-935d-799d68fe2684",
   "metadata": {},
   "source": [
    "## 12. implement faster R-CNN using 2017 coco dataset(https://cocodataset.org/#download).i.e train dataset. val dataset and test dataset you can use a pre-trained backbone network like resnet or VGG for feature extraction for reference implement the following steps:\n",
    "## a. Dataset Preparatoin:\n",
    "## i. Dwnlad and preprcess the coco dataset, including the annotatins and images.\n",
    "## ii. Split the dataset into training and validatoin sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accbcc05-0f0b-4953-9968-815536ea4aed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd77e895-239a-4f4e-b84f-0439fba0edb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ultralytics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load a model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov8n.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# load a pretrained model (recommended for training)\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ultralytics'"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Train the model\n",
    "results = model.train(data='coco.yaml', epochs=100, imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f50650f-f8aa-4a50-9073-54903e0afb0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fiftyone' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mfiftyone\u001b[49m\u001b[38;5;241m.\u001b[39mzoo\u001b[38;5;241m.\u001b[39mload_zoo_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoco-2017\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fiftyone' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = fiftyone.zoo.load_zoo_dataset(\"coco-2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3bd41b2-2010-4fe5-a7c5-e302aaa7c5bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fiftyone' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mfiftyone\u001b[49m\u001b[38;5;241m.\u001b[39mzoo\u001b[38;5;241m.\u001b[39mload_zoo_dataset(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoco-2017\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     label_types\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetections\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegmentations\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      5\u001b[0m     classes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperson\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcar\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      6\u001b[0m     max_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Visualize the dataset in the FiftyOne App\u001b[39;00m\n\u001b[1;32m     10\u001b[0m session \u001b[38;5;241m=\u001b[39m fiftyone\u001b[38;5;241m.\u001b[39mlaunch_app(dataset)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fiftyone' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = fiftyone.zoo.load_zoo_dataset(\n",
    "    \"coco-2017\",\n",
    "    split=\"validation\",\n",
    "    label_types=[\"detections\", \"segmentations\"],\n",
    "    classes=[\"person\", \"car\"],\n",
    "    max_samples=50,\n",
    ")\n",
    "\n",
    "# Visualize the dataset in the FiftyOne App\n",
    "session = fiftyone.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18282f5-94a9-4b7c-a9f4-2433c01b5e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
